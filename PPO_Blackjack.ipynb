{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecF-suQG8gWO",
        "outputId": "99ba7093-9898-44a9-90fd-8d9688324f97"
      },
      "outputs": [],
      "source": [
        "%pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XM39ch85jur",
        "outputId": "9c6c18d7-ab6a-4486-90b1-02cdfcbc493e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Set up the Blackjack environment\n",
        "env = gym.make('Blackjack-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w1ARSO5gfS7y"
      },
      "outputs": [],
      "source": [
        "# PPO Actor-Critic Model\n",
        "class PPOModel(Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super(PPOModel, self).__init__()\n",
        "        self.common = layers.Dense(128, activation='relu')\n",
        "        self.actor = layers.Dense(num_actions, activation='softmax')\n",
        "        self.critic = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.common(inputs)\n",
        "        return self.actor(x), self.critic(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "U22cjSBkfo1o"
      },
      "outputs": [],
      "source": [
        "# Function to compute discounted rewards\n",
        "def compute_returns(rewards, dones, gamma=0.99):\n",
        "    returns = []\n",
        "    discounted_sum = 0\n",
        "    for reward, done in zip(reversed(rewards), reversed(dones)):\n",
        "        if done:\n",
        "            discounted_sum = 0\n",
        "        discounted_sum = reward + gamma * discounted_sum\n",
        "        returns.insert(0, discounted_sum)\n",
        "    return returns\n",
        "\n",
        "# Normalize advantages\n",
        "def normalize(x):\n",
        "    x -= np.mean(x)\n",
        "    x /= (np.std(x) + 1e-8)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "id": "C9jBe1KRYKZ0"
      },
      "outputs": [],
      "source": [
        "# Updated train_ppo function to accept hyperparameters\n",
        "def train_ppo(env, model, actor_optimizer, critic_optimizer, num_episodes=1000, clip_epsilon=0.2, update_epochs=10):\n",
        "    all_rewards = []\n",
        "    actor_losses = []\n",
        "    critic_losses = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"{episode} out of {num_episodes} training episodes complete\")\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, -1])\n",
        "        episode_states = []\n",
        "        episode_actions = []\n",
        "        episode_rewards = []\n",
        "        episode_dones = []\n",
        "\n",
        "        while True:\n",
        "            # Obtain the logits (action probabilities) and value (state value) from the model\n",
        "            logits, value = model(state)\n",
        "            # Choose an action based on the logits using a probability distribution\n",
        "            action = np.random.choice(env.action_space.n, p=np.squeeze(logits))\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            # Reshape the next state to be compatible with the model input\n",
        "            next_state = np.reshape(next_state, [1, -1])\n",
        "\n",
        "            episode_states.append(state)\n",
        "            episode_actions.append(action)\n",
        "            episode_rewards.append(reward)\n",
        "            episode_dones.append(done)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        # Compute the returns (discounted rewards) for the episode\n",
        "        returns = compute_returns(episode_rewards, episode_dones)\n",
        "        returns = np.array(returns).reshape(-1, 1)\n",
        "        states = np.vstack(episode_states)\n",
        "        actions = np.array(episode_actions)\n",
        "\n",
        "        for _ in range(update_epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Get the logits and values from the model for the collected states\n",
        "                logits, values = model(states)\n",
        "                values = tf.reshape(values, [-1])\n",
        "                advantage = returns - values\n",
        "\n",
        "                action_probs = tf.reduce_sum(tf.one_hot(actions, env.action_space.n) * logits, axis=1)\n",
        "                old_action_probs = action_probs  # Placeholder for old action probs\n",
        "\n",
        "                # Calculate the ratio of new and old action probabilities\n",
        "                ratios = tf.exp(tf.math.log(action_probs + 1e-10) - tf.math.log(old_action_probs + 1e-10))\n",
        "                # Calculate the surrogate loss for the policy update\n",
        "                surrogate1 = ratios * advantage\n",
        "                surrogate2 = tf.clip_by_value(ratios, 1 - clip_epsilon, 1 + clip_epsilon) * advantage\n",
        "\n",
        "                # Calculate the actor loss using the clipped surrogate loss\n",
        "                actor_loss = -tf.reduce_mean(tf.minimum(surrogate1, surrogate2))\n",
        "                # Calculate the critic loss as the mean squared error of returns and values\n",
        "                critic_loss = tf.reduce_mean(tf.square(returns - values))\n",
        "\n",
        "                total_loss = actor_loss + 0.5 * critic_loss\n",
        "\n",
        "            # Calculate the gradients of the total loss with respect to model parameters\n",
        "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "            actor_grads = grads[:len(model.common.trainable_variables) + len(model.actor.trainable_variables)]\n",
        "            critic_grads = grads[len(model.common.trainable_variables) + len(model.actor.trainable_variables):]\n",
        "\n",
        "            # Split the gradients into actor and critic gradients\n",
        "            actor_optimizer.apply_gradients(zip(actor_grads, model.trainable_variables[:len(model.common.trainable_variables) + len(model.actor.trainable_variables)]))\n",
        "            critic_optimizer.apply_gradients(zip(critic_grads, model.trainable_variables[len(model.common.trainable_variables) + len(model.actor.trainable_variables):]))\n",
        "\n",
        "        all_rewards.append(np.sum(episode_rewards))\n",
        "        actor_losses.append(actor_loss.numpy())\n",
        "        critic_losses.append(critic_loss.numpy())\n",
        "\n",
        "    return all_rewards, actor_losses, critic_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "WWFV_yWmhpJa",
        "outputId": "8f0c6290-8d82-4443-b4e4-34ec83f33211"
      },
      "outputs": [],
      "source": [
        "# Function to train PPO with given hyperparameters\n",
        "def train_ppo_with_params(env, params, num_episodes=500):\n",
        "    actor_optimizer = optimizers.Adam(learning_rate=params['actor_learning_rate'])\n",
        "    critic_optimizer = optimizers.Adam(learning_rate=params['critic_learning_rate'])\n",
        "\n",
        "    model = PPOModel(env.action_space.n)\n",
        "\n",
        "    all_rewards, actor_losses, critic_losses = train_ppo(\n",
        "        env,\n",
        "        model,\n",
        "        actor_optimizer,\n",
        "        critic_optimizer,\n",
        "        num_episodes=num_episodes,\n",
        "        clip_epsilon=params['clip_epsilon'],\n",
        "        update_epochs=params['update_epochs']\n",
        "    )\n",
        "\n",
        "    avg_reward = np.mean(all_rewards[-100:])  # Average reward over the last 100 episodes\n",
        "    return avg_reward\n",
        "\n",
        "# Define the grid of hyperparameters\n",
        "param_grid = {\n",
        "    'actor_learning_rate': [0.0003, 0.001, 0.003],\n",
        "    'critic_learning_rate': [0.0003, 0.001, 0.003],\n",
        "    'gamma': [0.95, 0.99, 0.999],\n",
        "    'clip_epsilon': [0.1, 0.2, 0.3],\n",
        "    'update_epochs': [5, 10, 20]\n",
        "}\n",
        "\n",
        "# Grid search for hyperparameter tuning\n",
        "best_avg_reward = -float('inf')\n",
        "best_params = None\n",
        "\n",
        "# Evaluate each combination of hyperparameters\n",
        "for params in ParameterGrid(param_grid):\n",
        "    print(f\"Evaluating hyperparameters: {params}\")\n",
        "    avg_reward = train_ppo_with_params(env, params)\n",
        "    if avg_reward > best_avg_reward:\n",
        "        best_avg_reward = avg_reward\n",
        "        best_params = params\n",
        "\n",
        "print(f\"Best Parameters: {best_params}, Best Average Reward: {best_avg_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tV_oQi8i8oN9"
      },
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "def plot_training_statistics(rewards, actor_losses, critic_losses, wins, draws, losses):\n",
        "    episodes = range(len(rewards))\n",
        "    total_games = np.array(wins) + np.array(draws) + np.array(losses)\n",
        "    win_percentages = np.array(wins) / total_games * 100\n",
        "    draw_percentages = np.array(draws) / total_games * 100\n",
        "    loss_percentages = np.array(losses) / total_games * 100\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(episodes, rewards)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Total Rewards')\n",
        "    plt.title('Total Rewards per Episode')\n",
        "\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.plot(episodes, actor_losses)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Actor Loss')\n",
        "    plt.title('Actor Loss per Episode')\n",
        "\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.plot(episodes, critic_losses)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Critic Loss')\n",
        "    plt.title('Critic Loss per Episode')\n",
        "\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.plot(episodes, win_percentages, label='Win %')\n",
        "    plt.plot(episodes, draw_percentages, label='Draw %')\n",
        "    plt.plot(episodes, loss_percentages, label='Loss %')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Percentage')\n",
        "    plt.title('Win/Draw/Loss Percentages')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_HSTZ1qQhqyN"
      },
      "outputs": [],
      "source": [
        "# Train the agent with best hyperparameters and track win/draw/loss percentages\n",
        "def train_ppo_with_tracking(env, model, actor_optimizer, critic_optimizer, num_episodes=1000, clip_epsilon=0.2, update_epochs=10):\n",
        "    all_rewards = []\n",
        "    actor_losses = []\n",
        "    critic_losses = []\n",
        "    win_counts = []\n",
        "    draw_counts = []\n",
        "    loss_counts = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"{episode} out of {num_episodes} training episodes complete.\")\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, -1])\n",
        "        episode_states = []\n",
        "        episode_actions = []\n",
        "        episode_rewards = []\n",
        "        episode_dones = []\n",
        "\n",
        "        wins = 0\n",
        "        draws = 0\n",
        "        losses = 0\n",
        "\n",
        "        while True:\n",
        "            # Obtain the logits (action probabilities) and value (state value) from the model\n",
        "            logits, value = model(state)\n",
        "            # Choose an action based on the logits using a probability distribution\n",
        "            action = np.random.choice(env.action_space.n, p=np.squeeze(logits))\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            # Reshape the next state to be compatible with the model input\n",
        "            next_state = np.reshape(next_state, [1, -1])\n",
        "\n",
        "            episode_states.append(state)\n",
        "            episode_actions.append(action)\n",
        "            episode_rewards.append(reward)\n",
        "            episode_dones.append(done)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                if reward > 0:\n",
        "                    wins += 1\n",
        "                elif reward == 0:\n",
        "                    draws += 1\n",
        "                else:\n",
        "                    losses += 1\n",
        "                break\n",
        "\n",
        "        # Compute the returns (discounted rewards) for the episode\n",
        "        returns = compute_returns(episode_rewards, episode_dones)\n",
        "        returns = np.array(returns).reshape(-1, 1)\n",
        "        states = np.vstack(episode_states)\n",
        "        actions = np.array(episode_actions)\n",
        "\n",
        "        for _ in range(update_epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Get the logits and values from the model for the collected states\n",
        "                logits, values = model(states)\n",
        "                values = tf.reshape(values, [-1])\n",
        "                advantage = returns - values\n",
        "\n",
        "                # Calculate action probabilities for the taken actions\n",
        "                action_probs = tf.reduce_sum(tf.one_hot(actions, env.action_space.n) * logits, axis=1)\n",
        "                old_action_probs = action_probs  # Placeholder for old action probs\n",
        "\n",
        "                # Calculate the ratio of new and old action probabilities\n",
        "                ratios = tf.exp(tf.math.log(action_probs + 1e-10) - tf.math.log(old_action_probs + 1e-10))\n",
        "                # Calculate the surrogate loss for the policy update\n",
        "                surrogate1 = ratios * advantage\n",
        "                surrogate2 = tf.clip_by_value(ratios, 1 - clip_epsilon, 1 + clip_epsilon) * advantage\n",
        "\n",
        "                # Calculate the actor loss using the clipped surrogate loss\n",
        "                actor_loss = -tf.reduce_mean(tf.minimum(surrogate1, surrogate2))\n",
        "                # Calculate the critic loss as the mean squared error of returns and values\n",
        "                critic_loss = tf.reduce_mean(tf.square(returns - values))\n",
        "\n",
        "                total_loss = actor_loss + 0.5 * critic_loss\n",
        "\n",
        "            # Calculate the gradients of the total loss with respect to model parameter\n",
        "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "            # Split the gradients into actor and critic gradients\n",
        "            actor_grads = grads[:len(model.common.trainable_variables) + len(model.actor.trainable_variables)]\n",
        "            critic_grads = grads[len(model.common.trainable_variables) + len(model.actor.trainable_variables):]\n",
        "\n",
        "            # optimizer\n",
        "            actor_optimizer.apply_gradients(zip(actor_grads, model.trainable_variables[:len(model.common.trainable_variables) + len(model.actor.trainable_variables)]))\n",
        "            critic_optimizer.apply_gradients(zip(critic_grads, model.trainable_variables[len(model.common.trainable_variables) + len(model.actor.trainable_variables):]))\n",
        "\n",
        "        all_rewards.append(np.sum(episode_rewards))\n",
        "        actor_losses.append(actor_loss.numpy())\n",
        "        critic_losses.append(critic_loss.numpy())\n",
        "        win_counts.append(wins)\n",
        "        draw_counts.append(draws)\n",
        "        loss_counts.append(losses)\n",
        "\n",
        "    return model, all_rewards, actor_losses, critic_losses, win_counts, draw_counts, loss_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "YcdTLDNxh0MK",
        "outputId": "5b2ff7be-8977-4188-f058-02a7861d7b77"
      },
      "outputs": [],
      "source": [
        "best_params = {'actor_learning_rate': 0.0003, 'clip_epsilon': 0.1, 'critic_learning_rate': 0.0003, 'update_epochs': 10}\n",
        "# Train the agent with best hyperparameters\n",
        "best_actor_optimizer = optimizers.Adam(learning_rate=best_params['actor_learning_rate'])\n",
        "best_critic_optimizer = optimizers.Adam(learning_rate=best_params['critic_learning_rate'])\n",
        "\n",
        "best_model = PPOModel(env.action_space.n)\n",
        "\n",
        "model, best_rewards, best_actor_losses, best_critic_losses, win_counts, draw_counts, loss_counts = train_ppo_with_tracking(\n",
        "    env,\n",
        "    best_model,\n",
        "    best_actor_optimizer,\n",
        "    best_critic_optimizer,\n",
        "    num_episodes=1000,\n",
        "    clip_epsilon=best_params['clip_epsilon'],\n",
        "    update_epochs=best_params['update_epochs']\n",
        ")\n",
        "\n",
        "# Plot the results\n",
        "plot_training_statistics(best_rewards, best_actor_losses, best_critic_losses, win_counts, draw_counts, loss_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTnFkOtP9RaH",
        "outputId": "3463e08a-f4ad-4b89-ae81-458ccc25618f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Player's hand: [17]\n",
            "Dealer's visible hand: 10\n",
            "Loss! :(  -1.0 \n",
            "\n",
            "Player's hand: [13]\n",
            "Dealer's visible hand: 7\n",
            "Loss! :(  -1.0 \n",
            "\n",
            "Player's hand: [15]\n",
            "Dealer's visible hand: 2\n",
            "Win! :)  1.0 \n",
            "\n",
            "Player's hand: [14]\n",
            "Dealer's visible hand: 2\n",
            "Loss! :(  -1.0 \n",
            "\n",
            "Player's hand: [16]\n",
            "Dealer's visible hand: 7\n",
            "Loss! :(  -1.0 \n",
            "\n",
            "Player's hand: [10]\n",
            "Dealer's visible hand: 6\n",
            "Loss! :(  -1.0 \n",
            "\n",
            "Player's hand: [19]\n",
            "Dealer's visible hand: 8\n",
            "Win! :)  1.0 \n",
            "\n",
            "Player's hand: [14]\n",
            "Dealer's visible hand: 1\n",
            "Loss! :(  -1.0 \n",
            "\n",
            "Player's hand: [10]\n",
            "Dealer's visible hand: 9\n",
            "Loss! :(  -1.0 \n",
            "\n",
            "Player's hand: [5, 16]\n",
            "Dealer's visible hand: 10\n",
            "Loss! :(  -1.0 \n",
            "\n",
            "Win percentage:  20.0 %\n",
            "Draw percentage:  0.0 %\n",
            "Loss percentage:  80.0 %\n"
          ]
        }
      ],
      "source": [
        "# Parameters for testing\n",
        "games = 10\n",
        "wins = 0\n",
        "losses = 0\n",
        "draws = 0\n",
        "verbose = True\n",
        "\n",
        "for i in range(games):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    rewards = 0\n",
        "    player_hand = []\n",
        "    dealer_hand = []\n",
        "\n",
        "    # Get the initial player and dealer cards\n",
        "    player_hand.append(state[0])  # Player's first card\n",
        "    dealer_visible_card = state[1]  # Dealer's visible card (1-10 where 1 is ace)\n",
        "    dealer_hand.append(dealer_visible_card)\n",
        "\n",
        "    while not done:\n",
        "        state_array = np.array(state).reshape(1, -1)\n",
        "        action_probs, _ = model(state_array)\n",
        "        action = np.argmax(action_probs.numpy()[0])\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        rewards += reward\n",
        "\n",
        "        # Update the player's hand if the action was hit\n",
        "        if action == 1:  # Hit\n",
        "            player_hand.append(state[0])\n",
        "\n",
        "    if verbose:\n",
        "        # Print the result of the game\n",
        "        print(\"Player's hand:\", player_hand)\n",
        "        print(\"Dealer's visible hand:\", dealer_visible_card)\n",
        "\n",
        "    if rewards > 0:\n",
        "        print(\"Win! :) \", rewards, \"\\n\")\n",
        "        wins += 1\n",
        "    elif rewards == 0:\n",
        "        print(\"Draw! :| \", rewards, \"\\n\")\n",
        "        draws += 1\n",
        "    else:\n",
        "        print(\"Loss! :( \", rewards, \"\\n\")\n",
        "        losses += 1\n",
        "\n",
        "print(\"Win percentage: \", (wins/games)*100, \"%\")\n",
        "print(\"Draw percentage: \", (draws/games)*100, \"%\")\n",
        "print(\"Loss percentage: \", (losses/games)*100, \"%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
